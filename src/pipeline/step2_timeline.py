"""
Step 2: æ—¶é—´ç‚¹æå– - åŸºäºå¤§çº²å’ŒSRTå®šä½è¯é¢˜æ—¶é—´åŒºé—´
"""
import json
import logging
from typing import List, Dict, Optional
from pathlib import Path
from collections import defaultdict

from ..utils.llm_factory import LLMFactory
from ..utils.text_processor import TextProcessor
from ..config import PROMPT_FILES, METADATA_DIR

logger = logging.getLogger(__name__)

class TimelineExtractor:
    """ä»å¤§çº²å’ŒSRTå­—å¹•ä¸­æå–ç²¾ç¡®æ—¶é—´çº¿"""
    
    def __init__(self, metadata_dir: Path = None, prompt_files: Dict = None):
        self.llm_client = LLMFactory.get_default_client()
        self.text_processor = TextProcessor()
        
        # ä½¿ç”¨ä¼ å…¥çš„metadata_diræˆ–é»˜è®¤å€¼
        if metadata_dir is None:
            metadata_dir = METADATA_DIR
        self.metadata_dir = metadata_dir
        
        # åŠ è½½æç¤ºè¯
        prompt_files_to_use = prompt_files if prompt_files is not None else PROMPT_FILES
        with open(prompt_files_to_use['timeline'], 'r', encoding='utf-8') as f:
            self.timeline_prompt = f.read()
            
        # SRTå—çš„ç›®å½•
        self.srt_chunks_dir = self.metadata_dir / "step1_srt_chunks"
        self.timeline_chunks_dir = self.metadata_dir / "step2_timeline_chunks"
        self.llm_raw_output_dir = self.metadata_dir / "step2_llm_raw_output"

    def extract_timeline(self, outlines: List[Dict]) -> List[Dict]:
        """
        æå–è¯é¢˜æ—¶é—´åŒºé—´ã€‚
        æ–°ç‰ˆç‰¹æ€§ï¼š
        - åŸºäºé¢„å…ˆåˆ†å—çš„SRT
        - æŒ‰å—æ‰¹é‡å¤„ç†
        - ç¼“å­˜åŸå§‹LLMå“åº”ï¼Œé¿å…é‡å¤è°ƒç”¨
        - ä¿å­˜æ¯ä¸ªå—çš„å¤„ç†ç»“æœä½œä¸ºä¸­é—´æ–‡ä»¶ï¼Œå¢å¼ºå¥å£®æ€§
        """
        logger.info("å¼€å§‹æå–è¯é¢˜æ—¶é—´åŒºé—´...")
        
        if not outlines:
            logger.warning("å¤§çº²æ•°æ®ä¸ºç©ºï¼Œæ— æ³•æå–æ—¶é—´çº¿ã€‚")
            return []

        if not self.srt_chunks_dir.exists():
            logger.error(f"SRTå—ç›®å½•ä¸å­˜åœ¨: {self.srt_chunks_dir}ã€‚è¯·å…ˆè¿è¡ŒStep 1ã€‚")
            return []

        # 1. åˆ›å»ºæœ¬æ­¥éª¤éœ€è¦çš„ç›®å½•
        self.timeline_chunks_dir.mkdir(parents=True, exist_ok=True)
        self.llm_raw_output_dir.mkdir(parents=True, exist_ok=True)

        # 2. æŒ‰ chunk_index å¯¹æ‰€æœ‰å¤§çº²è¿›è¡Œåˆ†ç»„
        outlines_by_chunk = defaultdict(list)
        for outline in outlines:
            chunk_index = outline.get('chunk_index')
            if chunk_index is not None:
                outlines_by_chunk[chunk_index].append(outline)
            else:
                logger.warning(f"  > è¯é¢˜ '{outline.get('title', 'æœªçŸ¥')}' ç¼ºå°‘ chunk_indexï¼Œå°†è¢«è·³è¿‡ã€‚")

        all_timeline_data = []
        # 3. éå†æ¯ä¸ªå—ï¼Œæ‰¹é‡å¤„ç†ï¼Œå¹¶å°†ç»“æœå­˜ä¸ºç‹¬ç«‹çš„JSONæ–‡ä»¶
        for chunk_index, chunk_outlines in outlines_by_chunk.items():
            logger.info(f"å¤„ç†å— {chunk_index}ï¼Œå…¶ä¸­åŒ…å« {len(chunk_outlines)} ä¸ªè¯é¢˜...")
            
            # æ¯æ¬¡éƒ½é‡æ–°å¤„ç†ï¼Œä¸ä½¿ç”¨ç¼“å­˜
            chunk_output_path = self.timeline_chunks_dir / f"chunk_{chunk_index}.json"

            try:
                # é¦–å…ˆåŠ è½½å¯¹åº”çš„SRTå—æ–‡ä»¶ï¼Œæ— è®ºæ˜¯å¦ä½¿ç”¨ç¼“å­˜éƒ½éœ€è¦è¿™äº›ä¿¡æ¯
                srt_chunk_path = self.srt_chunks_dir / f"chunk_{chunk_index}.json"
                if not srt_chunk_path.exists():
                    logger.warning(f"  > æ‰¾ä¸åˆ°å¯¹åº”çš„SRTå—æ–‡ä»¶: {srt_chunk_path}ï¼Œè·³è¿‡æ•´ä¸ªå—ã€‚")
                    continue
                
                with open(srt_chunk_path, 'r', encoding='utf-8') as f:
                    srt_chunk_data = json.load(f)

                if not srt_chunk_data:
                    logger.warning(f"  > SRTå—æ–‡ä»¶ä¸ºç©º: {srt_chunk_path}ï¼Œè·³è¿‡æ•´ä¸ªå—ã€‚")
                    continue

                # è·å–æ—¶é—´èŒƒå›´ä¿¡æ¯
                chunk_start_time = srt_chunk_data[0]['start_time']
                chunk_end_time = srt_chunk_data[-1]['end_time']

                raw_response = ""
                llm_cache_path = self.llm_raw_output_dir / f"chunk_{chunk_index}.txt"

                if llm_cache_path.exists():
                    logger.info(f"  > æ‰¾åˆ°å— {chunk_index} çš„LLMåŸå§‹å“åº”ç¼“å­˜ï¼Œç›´æ¥è¯»å–ã€‚")
                    with open(llm_cache_path, 'r', encoding='utf-8') as f:
                        raw_response = f.read()
                else:
                    logger.info(f"  > æœªæ‰¾åˆ°LLMç¼“å­˜ï¼Œå¼€å§‹è°ƒç”¨API...")
                    
                    # æ„å»ºç”¨äºLLMçš„SRTæ–‡æœ¬
                    srt_text_for_prompt = ""
                    for sub in srt_chunk_data:
                        srt_text_for_prompt += f"{sub['index']}\\n{sub['start_time']} --> {sub['end_time']}\\n{sub['text']}\\n\\n"
                    
                    # ä¸ºLLMå‡†å¤‡ä¸€ä¸ª"å¹²å‡€"çš„è¾“å…¥ï¼ŒåªåŒ…å«å®ƒéœ€è¦çš„ä¿¡æ¯
                    llm_input_outlines = [
                        {"title": o.get("title"), "subtopics": o.get("subtopics")}
                        for o in chunk_outlines
                    ]

                    input_data = {
                        "outline": llm_input_outlines,  # ä½¿ç”¨å¹²å‡€çš„æ•°æ®
                        "srt_text": srt_text_for_prompt
                    }
                    
                    # è°ƒç”¨LLMè·å–åŸå§‹å“åº”ï¼Œå¸¦é‡è¯•æœºåˆ¶
                    parsed_items = None
                    max_parse_retries = 2
                    
                    for retry_count in range(max_parse_retries + 1):
                        try:
                            logger.info(f"  > ğŸš€ [å— {chunk_index} ç¬¬{retry_count + 1}æ¬¡å°è¯•] è°ƒç”¨LLMæå–æ—¶é—´è½´...")
                            logger.info(f"  > ğŸ“Š [è¾“å…¥ç»Ÿè®¡] å¤§çº²æ•°é‡: {len(llm_input_outlines)}, SRTæ¡ç›®: {len(srt_chunk_data)}")
                            logger.debug(f"  > ğŸ“„ [è¾“å…¥è¯¦æƒ…] SRTæ–‡æœ¬å‰300å­—ç¬¦: {srt_text_for_prompt[:300]}...")
                            
                            raw_response = self.llm_client.call_with_retry(self.timeline_prompt, input_data)
                            
                            if not raw_response:
                                logger.warning(f"  > âš ï¸ [å— {chunk_index} ç©ºå“åº”] LLMå“åº”ä¸ºç©ºï¼Œè·³è¿‡")
                                break
                            
                            logger.info(f"  > âœ… [å— {chunk_index} å“åº”æˆåŠŸ] è·å¾—LLMå“åº”ï¼Œé•¿åº¦: {len(raw_response)} å­—ç¬¦")
                            
                            # ä¿å­˜åŸå§‹å“åº”åˆ°ç¼“å­˜
                            cache_file = self.llm_raw_output_dir / f"chunk_{chunk_index}_attempt_{retry_count}.txt"
                            with open(cache_file, 'w', encoding='utf-8') as f:
                                f.write(raw_response)
                            logger.info(f"  > ğŸ’¾ [ç¼“å­˜ä¿å­˜] åŸå§‹å“åº”å·²ä¿å­˜åˆ°: {cache_file.name}")
                            
                            # è§£æLLMçš„åŸå§‹å“åº”
                            parsed_items = self._parse_and_validate_response(
                                raw_response, 
                                chunk_start_time, 
                                chunk_end_time,
                                chunk_index
                            )
                            
                            if parsed_items:
                                # ä¿å­˜è§£æåçš„ç»“æœ
                                with open(chunk_output_path, 'w', encoding='utf-8') as f:
                                    json.dump(parsed_items, f, ensure_ascii=False, indent=2)
                                
                                logger.info(f"  > å— {chunk_index} æˆåŠŸè§£æ {len(parsed_items)} ä¸ªæ—¶é—´æ®µ")
                                break  # æˆåŠŸè§£æï¼Œè·³å‡ºé‡è¯•å¾ªç¯
                            else:
                                if retry_count < max_parse_retries:
                                    logger.warning(f"  > å— {chunk_index} è§£æå¤±è´¥ï¼Œå°è¯•é‡è¯• ({retry_count + 1}/{max_parse_retries + 1})")
                                    # åœ¨é‡è¯•æ—¶å¼ºåŒ–æç¤ºè¯ï¼Œå¼ºè°ƒJSONæ ¼å¼
                                    input_data['additional_instruction'] = "\n\nã€é‡è¦ã€‘è¾“å‡ºè¦æ±‚ï¼š\n1. å¿…é¡»ä»¥[å¼€å§‹ï¼Œä»¥]ç»“æŸ\n2. ä½¿ç”¨è‹±æ–‡åŒå¼•å·ï¼Œä¸è¦ä½¿ç”¨ä¸­æ–‡å¼•å·\n3. å­—ç¬¦ä¸²ä¸­çš„å¼•å·å¿…é¡»è½¬ä¹‰ä¸º\\\"\n4. ä¸è¦æ·»åŠ ä»»ä½•è§£é‡Šæ–‡å­—æˆ–ä»£ç å—æ ‡è®°\n5. ç¡®ä¿JSONæ ¼å¼å®Œå…¨æ­£ç¡®"
                                else:
                                    logger.error(f"  > å— {chunk_index} ç»è¿‡ {max_parse_retries + 1} æ¬¡å°è¯•ä»ç„¶è§£æå¤±è´¥")
                                    # ä¿å­˜æœ€åä¸€æ¬¡çš„åŸå§‹å“åº”ä»¥ä¾¿è°ƒè¯•
                                    self._save_debug_response(raw_response, chunk_index, "final_parse_failure")
                                    
                        except Exception as parse_error:
                            logger.error(f"  > å— {chunk_index} ç¬¬ {retry_count + 1} æ¬¡å°è¯•è§£æè¿‡ç¨‹ä¸­å‘ç”Ÿå¼‚å¸¸: {parse_error}")
                            if retry_count == max_parse_retries:
                                # ä¿å­˜åŸå§‹å“åº”ä»¥ä¾¿è°ƒè¯•
                                self._save_debug_response(raw_response if 'raw_response' in locals() else "No response", chunk_index, "parse_exception")
                            continue
                    
                    if not parsed_items:
                         logger.warning(f"  > å— {chunk_index} æœ€ç»ˆè§£æå¤±è´¥ï¼Œè·³è¿‡")
                         continue

            except Exception as e:
                logger.error(f"  > å¤„ç†å— {chunk_index} æ—¶å‡ºé”™: {str(e)}")
                continue
        
        # 4. ä»æ‰€æœ‰ä¸­é—´æ–‡ä»¶ä¸­æ‹¼æ¥æœ€ç»ˆç»“æœ
        logger.info("æ‰€æœ‰å—å¤„ç†å®Œæ¯•ï¼Œå¼€å§‹ä»ä¸­é—´æ–‡ä»¶æ‹¼æ¥æœ€ç»ˆç»“æœ...")
        all_timeline_data = []
        chunk_files = sorted(self.timeline_chunks_dir.glob("*.json"))
        for chunk_file in chunk_files:
            with open(chunk_file, 'r', encoding='utf-8') as f:
                chunk_data = json.load(f)
                all_timeline_data.extend(chunk_data)

        logger.info(f"æˆåŠŸä» {len(chunk_files)} ä¸ªå—æ–‡ä»¶ä¸­åŠ è½½äº† {len(all_timeline_data)} ä¸ªè¯é¢˜ã€‚")
        
        # æœ€ç»ˆæ’åºï¼šåœ¨è¿”å›æ‰€æœ‰ç»“æœå‰ï¼ŒæŒ‰å¼€å§‹æ—¶é—´è¿›è¡Œå…¨å±€æ’åº
        if all_timeline_data:
            logger.info("æŒ‰å¼€å§‹æ—¶é—´å¯¹æ‰€æœ‰è¯é¢˜è¿›è¡Œæœ€ç»ˆæ’åº...")
            try:
                # ä½¿ç”¨ text_processor å°†æ—¶é—´å­—ç¬¦ä¸²è½¬æ¢ä¸ºç§’æ•°ä»¥ä¾¿æ­£ç¡®æ’åº
                all_timeline_data.sort(key=lambda x: self.text_processor.time_to_seconds(x['start_time']))
                logger.info("æ’åºå®Œæˆã€‚")
                
                # ä¸ºæ‰€æœ‰ç‰‡æ®µæŒ‰æ—¶é—´é¡ºåºåˆ†é…å›ºå®šçš„ID
                logger.info("ä¸ºæ‰€æœ‰ç‰‡æ®µæŒ‰æ—¶é—´é¡ºåºåˆ†é…å›ºå®šID...")
                for i, timeline_item in enumerate(all_timeline_data):
                    timeline_item['id'] = str(i + 1)
                logger.info(f"å·²ä¸º {len(all_timeline_data)} ä¸ªç‰‡æ®µåˆ†é…äº†å›ºå®šIDï¼ˆ1-{len(all_timeline_data)}ï¼‰")
                
            except Exception as e:
                logger.error(f"å¯¹æœ€ç»ˆç»“æœæ’åºæ—¶å‡ºé”™: {e}ã€‚è¿”å›æœªæ’åºçš„ç»“æœã€‚")

        return all_timeline_data
        
    def _parse_and_validate_response(self, response: str, chunk_start: str, chunk_end: str, chunk_index: int) -> List[Dict]:
        """å¢å¼ºçš„è§£æLLMçš„æ‰¹é‡å“åº”ã€éªŒè¯å¹¶è°ƒæ•´æ—¶é—´"""
        validated_items = []
        
        # ä¿å­˜åŸå§‹å“åº”ç”¨äºè°ƒè¯•
        self._save_debug_response(response, chunk_index, "original_response")
        
        try:
            # å°è¯•è§£æJSON
            parsed_response = self.llm_client.parse_json_response(response)
            
            # éªŒè¯JSONç»“æ„
            if not self.llm_client._validate_json_structure(parsed_response):
                logger.error(f"  > å— {chunk_index} JSONç»“æ„éªŒè¯å¤±è´¥")
                self._save_debug_response(str(parsed_response), chunk_index, "invalid_structure")
                return []
            
            if not isinstance(parsed_response, list):
                logger.warning(f"  > å— {chunk_index} LLMè¿”å›çš„ä¸æ˜¯ä¸€ä¸ªåˆ—è¡¨")
                self._save_debug_response(f"ç±»å‹: {type(parsed_response)}, å†…å®¹: {parsed_response}", chunk_index, "not_list")
                return []
            
            for timeline_item in parsed_response:
                if 'outline' not in timeline_item or 'start_time' not in timeline_item or 'end_time' not in timeline_item:
                    logger.warning(f"  > ä»LLMè¿”å›çš„æŸä¸ªJSONå¯¹è±¡æ ¼å¼ä¸æ­£ç¡®: {timeline_item}")
                    continue
                
                # å°† chunk_index æ·»åŠ å›å¯¹è±¡ä¸­ï¼Œä»¥ä¾¿åç»­æ­¥éª¤ä½¿ç”¨
                timeline_item['chunk_index'] = chunk_index
                
                # éªŒè¯å’Œè°ƒæ•´æ—¶é—´èŒƒå›´
                try:
                    # éªŒè¯æ—¶é—´æ ¼å¼
                    if not self._validate_time_format(timeline_item['start_time']):
                        logger.warning(f"  > è¯é¢˜ '{timeline_item['outline']}' å¼€å§‹æ—¶é—´æ ¼å¼ä¸æ­£ç¡®: {timeline_item['start_time']}")
                        continue
                    
                    if not self._validate_time_format(timeline_item['end_time']):
                        logger.warning(f"  > è¯é¢˜ '{timeline_item['outline']}' ç»“æŸæ—¶é—´æ ¼å¼ä¸æ­£ç¡®: {timeline_item['end_time']}")
                        continue
                    
                    start_time = self._convert_time_format(timeline_item['start_time'])
                    end_time = self._convert_time_format(timeline_item['end_time'])
                    
                    start_sec = self.text_processor.time_to_seconds(start_time)
                    end_sec = self.text_processor.time_to_seconds(end_time)
                    chunk_start_sec = self.text_processor.time_to_seconds(chunk_start)
                    chunk_end_sec = self.text_processor.time_to_seconds(chunk_end)
                    
                    if start_sec < chunk_start_sec:
                        logger.warning(f"  > è°ƒæ•´è¯é¢˜ '{timeline_item['outline']}' çš„å¼€å§‹æ—¶é—´ä» {start_time} åˆ° {chunk_start}")
                        timeline_item['start_time'] = chunk_start
                    
                    if end_sec > chunk_end_sec:
                        logger.warning(f"  > è°ƒæ•´è¯é¢˜ '{timeline_item['outline']}' çš„ç»“æŸæ—¶é—´ä» {end_time} åˆ° {chunk_end}")
                        timeline_item['end_time'] = chunk_end
                    
                    logger.info(f"  > å®šä½æˆåŠŸ: {timeline_item['outline']} ({timeline_item['start_time']} -> {timeline_item['end_time']})")
                    validated_items.append(timeline_item)
                except Exception as e:
                    logger.error(f"  > éªŒè¯å•ä¸ªæ—¶é—´æˆ³æ—¶å‡ºé”™: {e} - é¡¹ç›®: {timeline_item}")
                    continue
            
            return validated_items

        except Exception as e:
            logger.error(f"  > å— {chunk_index} è§£æLLMå“åº”æ—¶å‡ºé”™: {e}")
            # ä¿å­˜è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯
            error_info = {
                "error": str(e),
                "error_type": type(e).__name__,
                "response_length": len(response),
                "response_preview": response[:200],
                "chunk_index": chunk_index,
                "chunk_start": chunk_start,
                "chunk_end": chunk_end
            }
            import json
            self._save_debug_response(json.dumps(error_info, indent=2, ensure_ascii=False), chunk_index, "parse_error")
            return []

    def _validate_time_format(self, time_str: str) -> bool:
        """
        éªŒè¯æ—¶é—´æ ¼å¼æ˜¯å¦æ­£ç¡® (HH:MM:SS,mmm)
        """
        import re
        pattern = r'^\d{2}:\d{2}:\d{2},\d{3}$'
        return bool(re.match(pattern, time_str))
    
    def _convert_time_format(self, time_str: str) -> str:
        """
        è½¬æ¢æ—¶é—´æ ¼å¼ï¼šSRTæ ¼å¼ -> FFmpegæ ¼å¼
        """
        if not time_str or time_str == "end":
            return time_str
        return time_str.replace(',', '.')

    def _save_debug_response(self, response: str, chunk_index: int, error_type: str) -> None:
        """ä¿å­˜è°ƒè¯•å“åº”åˆ°æ–‡ä»¶"""
        try:
            debug_dir = self.metadata_dir / "debug_responses"
            debug_dir.mkdir(parents=True, exist_ok=True)
            debug_file = debug_dir / f"chunk_{chunk_index}_{error_type}.txt"
            with open(debug_file, 'w', encoding='utf-8') as f:
                f.write(response)
            logger.info(f"è°ƒè¯•å“åº”å·²ä¿å­˜åˆ°: {debug_file}")
        except Exception as e:
            logger.error(f"ä¿å­˜è°ƒè¯•å“åº”å¤±è´¥: {e}")

    def save_timeline(self, timeline_data: List[Dict], output_path: Optional[Path] = None) -> Path:
        """
        ä¿å­˜æ—¶é—´åŒºé—´æ•°æ®
        """
        if output_path is None:
            output_path = METADATA_DIR / "step2_timeline.json"
        
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(timeline_data, f, ensure_ascii=False, indent=2)
            
        logger.info(f"æ—¶é—´æ•°æ®å·²ä¿å­˜åˆ°: {output_path}")
        return output_path

    def load_timeline(self, input_path: Path) -> List[Dict]:
        """
        ä»æ–‡ä»¶åŠ è½½æ—¶é—´æ•°æ®
        """
        with open(input_path, 'r', encoding='utf-8') as f:
            return json.load(f)

def run_step2_timeline(outline_path: Path, metadata_dir: Path = None, output_path: Optional[Path] = None, prompt_files: Dict = None) -> List[Dict]:
    """
    è¿è¡ŒStep 2: æ—¶é—´ç‚¹æå–
    """
    if metadata_dir is None:
        metadata_dir = METADATA_DIR
        
    extractor = TimelineExtractor(metadata_dir, prompt_files)
    
    # åŠ è½½å¤§çº²
    with open(outline_path, 'r', encoding='utf-8') as f:
        outlines = json.load(f)
        
    timeline_data = extractor.extract_timeline(outlines)
    
    # ä¿å­˜ç»“æœ
    if output_path is None:
        output_path = metadata_dir / "step2_timeline.json"
        
    extractor.save_timeline(timeline_data, output_path)
    
    return timeline_data