"""
Step 3: å†…å®¹è¯„åˆ† - å¯¹æ¯ä¸ªè¯é¢˜ç‰‡æ®µè¿›è¡Œå¤šç»´åº¦è¯„åˆ†
"""
import json
import logging
from typing import List, Dict, Optional, Tuple
from pathlib import Path
from collections import defaultdict

from ..utils.llm_factory import LLMFactory
from ..utils.text_processor import TextProcessor
from ..config import PROMPT_FILES, METADATA_DIR, MIN_SCORE_THRESHOLD

logger = logging.getLogger(__name__)

class ClipScorer:
    """å†…å®¹è¯„åˆ†å™¨"""
    
    def __init__(self, prompt_files: Dict = None):
        self.llm_client = LLMFactory.get_default_client()
        self.text_processor = TextProcessor()
        
        # åŠ è½½æç¤ºè¯
        prompt_files_to_use = prompt_files if prompt_files is not None else PROMPT_FILES
        with open(prompt_files_to_use['recommendation'], 'r', encoding='utf-8') as f:
            self.recommendation_prompt = f.read()
    
    def score_clips(self, timeline_data: List[Dict]) -> List[Dict]:
        """
        ä¸ºåˆ‡ç‰‡è¯„åˆ† (æ–°ç‰ˆï¼šæŒ‰å—æ‰¹é‡å¤„ç†ï¼Œå¹¶ä½¿ç”¨LLMè¿›è¡Œç»¼åˆè¯„ä¼°)
        """
        if not timeline_data:
            logger.warning("æ—¶é—´çº¿æ•°æ®ä¸ºç©ºï¼Œæ— æ³•è¯„åˆ†")
            return []
            
        logger.info(f"å¼€å§‹ä¸º {len(timeline_data)} ä¸ªåˆ‡ç‰‡è¿›è¡Œæ‰¹é‡è¯„åˆ†...")
        
        # 1. æŒ‰ chunk_index å¯¹æ‰€æœ‰ timeline æ•°æ®è¿›è¡Œåˆ†ç»„
        timeline_by_chunk = defaultdict(list)
        for item in timeline_data:
            chunk_index = item.get('chunk_index')
            if chunk_index is not None:
                timeline_by_chunk[chunk_index].append(item)
            else:
                logger.warning(f"  > è¯é¢˜ '{item.get('outline', 'æœªçŸ¥')}' ç¼ºå°‘ chunk_indexï¼Œå°†è¢«è·³è¿‡ã€‚")
        
        all_scored_clips = []
        # 2. éå†æ¯ä¸ªå—ï¼Œæ‰¹é‡å¤„ç†å…¶ä¸­çš„æ‰€æœ‰è¯é¢˜
        for chunk_index, chunk_items in timeline_by_chunk.items():
            logger.info(f"å¤„ç†å— {chunk_index}ï¼Œå…¶ä¸­åŒ…å« {len(chunk_items)} ä¸ªè¯é¢˜...")
            try:
                # 3. ä½¿ç”¨LLMè¿›è¡Œæ‰¹é‡è¯„ä¼°
                scored_chunk_items = self._get_llm_evaluation(chunk_items)
                
                if scored_chunk_items:
                    all_scored_clips.extend(scored_chunk_items)
                else:
                    logger.warning(f"å— {chunk_index} çš„LLMè¯„ä¼°è¿”å›ä¸ºç©ºï¼Œè·³è¿‡ã€‚")

            except Exception as e:
                logger.error(f"  > å¤„ç†å— {chunk_index} è¿›è¡Œè¯„åˆ†æ—¶å‡ºé”™: {str(e)}")
                continue

        # 4. æŒ‰æœ€ç»ˆå¾—åˆ†å¯¹æ‰€æœ‰ç»“æœè¿›è¡Œæ’åº
        if all_scored_clips:
            all_scored_clips.sort(key=lambda x: x.get('final_score', 0), reverse=True)
            # ä¿æŒStep 2åˆ†é…çš„å›ºå®šIDï¼Œä¸å†é‡æ–°åˆ†é…
            logger.info("æŒ‰è¯„åˆ†æ’åºå®Œæˆï¼Œä¿æŒåŸæœ‰å›ºå®šIDä¸å˜")
            
            # æœ€ç»ˆæŒ‰IDæ’åºï¼Œç¡®ä¿æ—¶é—´é¡ºåºçš„ä¸€è‡´æ€§
            all_scored_clips.sort(key=lambda x: int(x.get('id', 0)))
            logger.info("æŒ‰IDæ’åºå®Œæˆï¼Œä¿æŒæ—¶é—´é¡ºåº")
                
        logger.info("æ‰€æœ‰åˆ‡ç‰‡è¯„åˆ†å®Œæˆ")
        return all_scored_clips
    
    def _get_llm_evaluation(self, clips: List[Dict]) -> List[Dict]:
        """
        ä½¿ç”¨LLMè¿›è¡Œæ‰¹é‡è¯„ä¼°ï¼Œä¸ºæ¯ä¸ªclipæ·»åŠ  final_score å’Œ recommend_reason
        """
        try:
            # è¾“å…¥ç»™LLMçš„æ•°æ®ä¸éœ€è¦åŒ…å«æ‰€æœ‰å­—æ®µï¼Œåªç»™å¿…è¦çš„
            input_for_llm = [
                {
                    "outline": clip.get('outline'), 
                    "content": clip.get('content'),
                    "start_time": clip.get('start_time'),
                    "end_time": clip.get('end_time'),
                } for clip in clips
            ]
            
            response = self.llm_client.call_with_retry(self.recommendation_prompt, input_for_llm)
            
            logger.info(f"âœ… [è¯„åˆ†å“åº”æˆåŠŸ] è·å¾—LLMå“åº”ï¼Œé•¿åº¦: {len(response) if response else 0} å­—ç¬¦")
            logger.debug(f"ğŸ“„ [è¯„åˆ†å“åº”å†…å®¹]: {response[:300] if response else 'N/A'}...")
            
            logger.info(f"ğŸ” [å¼€å§‹è§£æ] è§£æLLMè¯„åˆ†å“åº”...")
            parsed_list = self.llm_client.parse_json_response(response)
            
            if not isinstance(parsed_list, list) or len(parsed_list) != len(clips):
                logger.error(f"âŒ [è¯„åˆ†ç»“æœé”™è¯¯] LLMè¿”å›çš„è¯„åˆ†ç»“æœæ•°é‡ä¸è¾“å…¥ä¸åŒ¹é…ã€‚è¾“å…¥: {len(clips)}, è¾“å‡º: {len(parsed_list) if isinstance(parsed_list, list) else 'éåˆ—è¡¨'}")
                logger.debug(f"ğŸ“„ [è¯„åˆ†è§£æç»“æœç±»å‹]: {type(parsed_list)}")
                if isinstance(parsed_list, list):
                    logger.debug(f"ğŸ“„ [è¯„åˆ†ç»“æœå‰3ä¸ª]: {parsed_list[:3]}")
                return []
                
            # å°†è¯„åˆ†ç»“æœåˆå¹¶å›åŸå§‹çš„clipsæ•°æ®
            for original_clip, llm_result in zip(clips, parsed_list):
                score = llm_result.get('final_score')
                reason = llm_result.get('recommend_reason')
                
                if score is None or reason is None:
                    logger.warning(f"LLMè¿”å›çš„æŸä¸ªç»“æœç¼ºå°‘scoreæˆ–reason: {llm_result}")
                    original_clip['final_score'] = 0.0
                    original_clip['recommend_reason'] = "è¯„ä¼°å¤±è´¥"
                else:
                    original_clip['final_score'] = round(float(score), 2)
                    original_clip['recommend_reason'] = reason
                    logger.info(f"  > è¯„åˆ†æˆåŠŸ: {original_clip.get('outline', '')[:20]}... [åˆ†æ•°: {score}]")

            return clips

        except Exception as e:
            logger.error(f"LLMæ‰¹é‡è¯„ä¼°å¤±è´¥: {e}")
            # å¦‚æœæ‰¹é‡å¤±è´¥ï¼Œä¸ºæ‰€æœ‰clipsæ ‡è®°ä¸ºå¤±è´¥
            for clip in clips:
                clip['final_score'] = 0.0
                clip['recommend_reason'] = "æ‰¹é‡è¯„ä¼°å¤±è´¥"
            return clips

    def save_scores(self, scored_clips: List[Dict], output_path: Path):
        """ä¿å­˜è¯„åˆ†ç»“æœ"""
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(scored_clips, f, ensure_ascii=False, indent=2)
        logger.info(f"è¯„åˆ†ç»“æœå·²ä¿å­˜åˆ°: {output_path}")

def run_step3_scoring(timeline_path: Path, metadata_dir: Path = None, output_path: Optional[Path] = None, prompt_files: Dict = None) -> List[Dict]:
    """
    è¿è¡ŒStep 3: å†…å®¹è¯„åˆ†ä¸ç­›é€‰
    
    Args:
        timeline_path: æ—¶é—´çº¿æ–‡ä»¶è·¯å¾„
        output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        prompt_files: è‡ªå®šä¹‰æç¤ºè¯æ–‡ä»¶
        
    Returns:
        é«˜åˆ†åˆ‡ç‰‡åˆ—è¡¨
    """
    # åŠ è½½æ—¶é—´çº¿æ•°æ®
    with open(timeline_path, 'r', encoding='utf-8') as f:
        timeline_data = json.load(f)
    
    # åˆ›å»ºè¯„åˆ†å™¨
    scorer = ClipScorer(prompt_files)
    
    # è¯„åˆ†
    scored_clips = scorer.score_clips(timeline_data)
    
    # ç­›é€‰é«˜åˆ†åˆ‡ç‰‡
    high_score_clips = [clip for clip in scored_clips if clip['final_score'] >= MIN_SCORE_THRESHOLD]
    
    # ä¿å­˜ç»“æœ
    if metadata_dir is None:
        metadata_dir = METADATA_DIR
    
    # ä¿å­˜æ‰€æœ‰è¯„åˆ†åçš„ç‰‡æ®µï¼ˆç”¨äºè°ƒè¯•å’Œåˆ†æï¼‰
    all_scored_path = metadata_dir / "step3_all_scored.json"
    scorer.save_scores(scored_clips, all_scored_path)
    
    # ä¿å­˜ç­›é€‰åçš„é«˜åˆ†ç‰‡æ®µï¼ˆç”¨äºåç»­æ­¥éª¤ï¼‰
    if output_path is None:
        output_path = metadata_dir / "step3_high_score_clips.json"
        
    scorer.save_scores(high_score_clips, output_path)
    
    return high_score_clips