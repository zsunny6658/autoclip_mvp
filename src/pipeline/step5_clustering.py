"""
Step 5: ä¸»é¢˜èšç±» - å°†ç›¸å…³ç‰‡æ®µèšåˆä¸ºåˆé›†
"""
import json
import logging
from typing import List, Dict, Optional
from pathlib import Path

from ..utils.llm_factory import LLMFactory
from ..config import PROMPT_FILES, METADATA_DIR, MAX_CLIPS_PER_COLLECTION

logger = logging.getLogger(__name__)

class ClusteringEngine:
    """ä¸»é¢˜èšç±»å¼•æ“"""
    
    def __init__(self, metadata_dir: Optional[Path] = None, prompt_files: Dict = None):
        self.llm_client = LLMFactory.get_default_client()
        
        # åŠ è½½æç¤ºè¯
        prompt_files_to_use = prompt_files if prompt_files is not None else PROMPT_FILES
        with open(prompt_files_to_use['clustering'], 'r', encoding='utf-8') as f:
            self.clustering_prompt = f.read()
        
        # ä½¿ç”¨ä¼ å…¥çš„metadata_diræˆ–é»˜è®¤å€¼
        if metadata_dir is None:
            metadata_dir = METADATA_DIR
        self.metadata_dir = metadata_dir
    
    def cluster_clips(self, clips_with_titles: List[Dict]) -> List[Dict]:
        """
        å¯¹ç‰‡æ®µè¿›è¡Œä¸»é¢˜èšç±»
        
        Args:
            clips_with_titles: å¸¦æ ‡é¢˜çš„ç‰‡æ®µåˆ—è¡¨
            
        Returns:
            åˆé›†æ•°æ®åˆ—è¡¨
        """
        logger.info("å¼€å§‹è¿›è¡Œä¸»é¢˜èšç±»...")
        
        # å‡†å¤‡èšç±»æ•°æ®
        clips_for_clustering = []
        for clip in clips_with_titles:
            clips_for_clustering.append({
                'id': clip['id'],
                'title': clip.get('generated_title', clip['outline']),
                'summary': clip.get('recommend_reason', ''),
                'score': clip.get('final_score', 0)
            })
        
        logger.info(f"ğŸ“Š [èšç±»æ•°æ®å‡†å¤‡] å‡†å¤‡èšç±»æ•°æ®ï¼Œå…±{len(clips_for_clustering)}ä¸ªç‰‡æ®µ")
        logger.debug(f"ğŸ“„ [ç‰‡æ®µæ•°æ®è¯¦æƒ…] ç‰‡æ®µåˆ—è¡¨: {json.dumps(clips_for_clustering, ensure_ascii=False, indent=2)[:1000]}...")
        
        # é¦–å…ˆè¿›è¡ŒåŸºäºå…³é”®è¯çš„é¢„èšç±»
        pre_clusters = self._pre_cluster_by_keywords(clips_for_clustering)
        
        # æ„å»ºå®Œæ•´çš„æç¤ºè¯
        full_prompt = self.clustering_prompt + "\n\nä»¥ä¸‹æ˜¯è§†é¢‘åˆ‡ç‰‡åˆ—è¡¨ï¼š\n"
        for i, clip in enumerate(clips_for_clustering, 1):
            full_prompt += f"{i}. æ ‡é¢˜ï¼š{clip['title']}\n   æ‘˜è¦ï¼š{clip['summary']}\n   è¯„åˆ†ï¼š{clip['score']:.2f}\n\n"
        
        # æ·»åŠ é¢„èšç±»ç»“æœä½œä¸ºå‚è€ƒ
        if pre_clusters:
            full_prompt += "\n\nåŸºäºå…³é”®è¯çš„é¢„èšç±»ç»“æœï¼ˆä»…ä¾›å‚è€ƒï¼‰ï¼š\n"
            for theme, clip_ids in pre_clusters.items():
                full_prompt += f"{theme}: {', '.join(clip_ids)}\n"
    
        try:
            # è°ƒç”¨å¤§æ¨¡å‹è¿›è¡Œèšç±»
            logger.info(f"ğŸš€ [èšç±»å¼€å§‹] è°ƒç”¨LLMè¿›è¡Œä¸»é¢˜èšç±»ï¼Œå¾…å¤„ç†ç‰‡æ®µæ•°: {len(clips_for_clustering)}")
            logger.info(f"ğŸ“„ [æç¤ºè¯é•¿åº¦]: {len(full_prompt)} å­—ç¬¦")
            logger.debug(f"ğŸ“„ [æç¤ºè¯é¢„è§ˆ]: {full_prompt[:500]}...")
            
            response = self.llm_client.call_with_retry(full_prompt)
            
            logger.info(f"âœ… [èšç±»å“åº”æˆåŠŸ] è·å¾—LLMå“åº”ï¼Œé•¿åº¦: {len(response) if response else 0} å­—ç¬¦")
            # è®°å½•å®Œæ•´çš„LLMå“åº”ç”¨äºè°ƒè¯•
            if response:
                logger.debug(f"ğŸ“„ [èšç±»å®Œæ•´å“åº”]: {response}")
            else:
                logger.warning("âš ï¸ [èšç±»å“åº”ä¸ºç©º] LLMè¿”å›ç©ºå“åº”")
                return self._create_default_collections(clips_with_titles)
            
            # è§£æJSONå“åº”
            logger.info(f"ğŸ” [å¼€å§‹è§£æ] è§£æLLMèšç±»å“åº”...")
            collections_data = self.llm_client.parse_json_response(response)
            
            # è®°å½•è§£æåçš„æ•°æ®
            logger.info(f"âœ… [è§£æå®Œæˆ] è§£æåçš„åˆé›†æ•°æ®æ•°é‡: {len(collections_data) if isinstance(collections_data, list) else 'N/A'}")
            if collections_data is not None:
                if isinstance(collections_data, list):
                    logger.debug(f"ğŸ“„ [è§£æåæ•°æ®é¢„è§ˆ]: {json.dumps(collections_data[:3] if len(collections_data) > 3 else collections_data, ensure_ascii=False, indent=2)}")
                else:
                    logger.warning(f"âš ï¸ [è§£ææ•°æ®ç±»å‹å¼‚å¸¸] è§£æç»“æœä¸æ˜¯åˆ—è¡¨ç±»å‹: {type(collections_data)}")
                    logger.debug(f"ğŸ“„ [è§£æç»“æœå†…å®¹]: {str(collections_data)[:500]}")
            else:
                logger.warning("âš ï¸ [è§£æç»“æœä¸ºç©º] è§£æåçš„collections_dataä¸ºNone")
                # å°è¯•ç›´æ¥è§£æresponse
                try:
                    direct_parse = json.loads(response)
                    logger.info(f"ğŸ”„ [ç›´æ¥è§£ææˆåŠŸ] ç›´æ¥è§£æresponseå¾—åˆ°çš„æ•°æ®ç±»å‹: {type(direct_parse)}")
                    logger.debug(f"ğŸ“„ [ç›´æ¥è§£ææ•°æ®]: {json.dumps(direct_parse, ensure_ascii=False, indent=2)[:500]}")
                    collections_data = direct_parse
                except json.JSONDecodeError as je:
                    logger.error(f"âŒ [ç›´æ¥è§£æå¤±è´¥] æ— æ³•ç›´æ¥è§£æresponseä¸ºJSON: {str(je)}")
                    logger.debug(f"ğŸ“„ [åŸå§‹å“åº”å†…å®¹]: {response}")
            
            # éªŒè¯å’Œæ¸…ç†åˆé›†æ•°æ®
            validated_collections = self._validate_collections(collections_data, clips_with_titles)
            
            # å¦‚æœLLMèšç±»ç»“æœä¸ç†æƒ³ï¼Œä½¿ç”¨é¢„èšç±»ç»“æœ
            if len(validated_collections) < 3:
                logger.warning("LLMèšç±»ç»“æœä¸ç†æƒ³ï¼Œä½¿ç”¨é¢„èšç±»ç»“æœ")
                validated_collections = self._create_collections_from_pre_clusters(pre_clusters, clips_with_titles)
            
            logger.info(f"ä¸»é¢˜èšç±»å®Œæˆï¼Œå…±{len(validated_collections)}ä¸ªåˆé›†")
            return validated_collections
            
        except Exception as e:
            logger.error(f"ä¸»é¢˜èšç±»å¤±è´¥: {str(e)}")
            logger.exception(e)
            # ä½¿ç”¨é¢„èšç±»ç»“æœä½œä¸ºå¤‡é€‰
            if pre_clusters:
                logger.info("ä½¿ç”¨é¢„èšç±»ç»“æœä½œä¸ºå¤‡é€‰æ–¹æ¡ˆ")
                return self._create_collections_from_pre_clusters(pre_clusters, clips_with_titles)
            # è¿”å›é»˜è®¤èšç±»ç»“æœ
            return self._create_default_collections(clips_with_titles)
    
    def _pre_cluster_by_keywords(self, clips: List[Dict]) -> Dict[str, List[str]]:
        """
        åŸºäºå…³é”®è¯è¿›è¡Œé¢„èšç±»
        
        Args:
            clips: ç‰‡æ®µåˆ—è¡¨
            
        Returns:
            é¢„èšç±»ç»“æœ
        """
        # å®šä¹‰ä¸»é¢˜å…³é”®è¯
        theme_keywords = {
            'æŠ•èµ„ç†è´¢': ['æŠ•èµ„', 'ç†è´¢', 'è‚¡ç¥¨', 'åŸºé‡‘', 'ç‚’è‚¡', 'èµšé’±', 'æ”¶ç›Š', 'æ¶¨è·Œ', 'è§£å¥—', 'æ•£æˆ·', 'Aè‚¡', 'åŒ—äº¤æ‰€', 'ä¸­å…', 'ç§ä¸š'],
            'èŒåœºæˆé•¿': ['èŒåœº', 'å·¥ä½œ', 'æŠ€èƒ½', 'å­¦ä¹ ', 'æ—¥è¯­', 'è‘£ç§˜', 'é€†è¢­', 'æ•™è‚²', 'å¤§å­¦ç”Ÿ', 'è´¢å•†'],
            'ç¤¾ä¼šè§‚å¯Ÿ': ['ç¤¾ä¼š', 'ç°è±¡', 'ç½‘ç»œ', 'ä¹±è±¡', 'åƒåœ¾', 'åˆ†ç±»', 'å¹³å°', 'æœºåˆ¶', 'ä¸»æ’­', 'è¡Œä¸š'],
            'æ–‡åŒ–å·®å¼‚': ['æ–‡åŒ–', 'å·®å¼‚', 'æ¬§ç¾', 'æ—¥æœ¬', 'éŸ©å›½', 'é¥®é£Ÿ', 'è¯­è¨€', 'ç‹è‡­', 'è’¸é”…', 'é‚®è½®'],
            'ç›´æ’­äº’åŠ¨': ['ç›´æ’­', 'äº’åŠ¨', 'å¼¹å¹•', 'ç²‰ä¸', 'èˆ°é•¿', 'æ‰“èµ', 'è¿éº¦', 'PK', 'æŠ½å¥–'],
            'æƒ…æ„Ÿå…³ç³»': ['æ‹çˆ±', 'æƒ…æ„Ÿ', 'ç¤¾äº¤', 'æ­è®ª', 'å…³ç³»', 'å¿ƒç†', 'å¿ƒåŠ¨', 'å†·æ·¡'],
            'å¥åº·ç”Ÿæ´»': ['å¥åº·', 'è¿åŠ¨', 'è·‘æ­¥', 'é¥®é£Ÿ', 'ç‰›å¥¶', 'ç”Ÿæ´»æ–¹å¼', 'é”»ç‚¼'],
            'åˆ›ä½œå¹³å°': ['åˆ›ä½œ', 'å¹³å°', 'Bç«™', 'å°çº¢ä¹¦', 'æ‘„å½±', 'å†…å®¹', 'è¿è¥', 'è‡ªåª’ä½“']
        }
        
        pre_clusters = {theme: [] for theme in theme_keywords.keys()}
        
        for clip in clips:
            # åˆå¹¶æ ‡é¢˜å’Œæ‘˜è¦è¿›è¡Œå…³é”®è¯åŒ¹é…
            text = f"{clip['title']} {clip['summary']}".lower()
            
            # è®¡ç®—æ¯ä¸ªä¸»é¢˜çš„åŒ¹é…åˆ†æ•°
            theme_scores = {}
            for theme, keywords in theme_keywords.items():
                score = sum(1 for keyword in keywords if keyword in text)
                if score > 0:
                    theme_scores[theme] = score
            
            # é€‰æ‹©åŒ¹é…åˆ†æ•°æœ€é«˜çš„ä¸»é¢˜
            if theme_scores:
                best_theme = max(theme_scores.keys(), key=lambda k: theme_scores[k])
                pre_clusters[best_theme].append(clip['id'])
        
        # è¿‡æ»¤æ‰ç©ºçš„ä¸»é¢˜
        return {theme: clip_ids for theme, clip_ids in pre_clusters.items() if len(clip_ids) >= 2}
    
    def _create_collections_from_pre_clusters(self, pre_clusters: Dict[str, List[str]], clips_with_titles: List[Dict]) -> List[Dict]:
        """
        ä»é¢„èšç±»ç»“æœåˆ›å»ºåˆé›†
        
        Args:
            pre_clusters: é¢„èšç±»ç»“æœ
            clips_with_titles: ç‰‡æ®µæ•°æ®
            
        Returns:
            åˆé›†æ•°æ®åˆ—è¡¨
        """
        collections = []
        collection_id = 1
        
        # ä¸»é¢˜æ ‡é¢˜æ˜ å°„
        theme_titles = {
            'æŠ•èµ„ç†è´¢': 'æŠ•èµ„ç†è´¢å¯ç¤º',
            'èŒåœºæˆé•¿': 'èŒåœºæˆé•¿è®°', 
            'ç¤¾ä¼šè§‚å¯Ÿ': 'ç¤¾ä¼šè§‚å¯Ÿç¬”è®°',
            'æ–‡åŒ–å·®å¼‚': 'æ–‡åŒ–å·®å¼‚è¶£è°ˆ',
            'ç›´æ’­äº’åŠ¨': 'ç›´æ’­äº’åŠ¨ç°åœº',
            'æƒ…æ„Ÿå…³ç³»': 'æƒ…æ„Ÿä¸å…³ç³»',
            'å¥åº·ç”Ÿæ´»': 'å¥åº·ç”Ÿæ´»æ–¹å¼',
            'åˆ›ä½œå¹³å°': 'åˆ›ä½œä¸å¹³å°ç”Ÿæ€'
        }
        
        # ä¸»é¢˜ç®€ä»‹æ˜ å°„
        theme_summaries = {
            'æŠ•èµ„ç†è´¢': 'é€šè¿‡ç”Ÿæ´»åŒ–æ¡ˆä¾‹åˆ†äº«æŠ•èµ„ç†å¿µï¼Œå…¼å…·å®ç”¨ä¸å…±é¸£ã€‚',
            'èŒåœºæˆé•¿': 'æ¢è®¨èŒä¸šå‘å±•ã€æŠ€èƒ½æå‡ä¸èŒåœºå¿ƒæ€å˜åŒ–ã€‚',
            'ç¤¾ä¼šè§‚å¯Ÿ': 'ç†æ€§ç‚¹è¯„ç¤¾ä¼šç°è±¡ä¸ç½‘ç»œä¹±è±¡ï¼Œè§‚ç‚¹é²œæ˜ã€‚',
            'æ–‡åŒ–å·®å¼‚': 'ä»é¥®é£Ÿåˆ°è¯­è¨€ï¼Œå±•ç°è·¨æ–‡åŒ–äº¤æµçš„è¶£å‘³è§†è§’ã€‚',
            'ç›´æ’­äº’åŠ¨': 'è¿˜åŸçœŸå®ç›´æ’­é—´äº’åŠ¨åœºæ™¯ï¼Œå±•ç°ä¸»æ’­ä¸´åœºååº”ã€‚',
            'æƒ…æ„Ÿå…³ç³»': 'è§£ææ‹çˆ±å¿ƒç†ã€ç¤¾äº¤å›°æƒ‘ä¸æƒ…æ„Ÿå…±é¸£è¯é¢˜ã€‚',
            'å¥åº·ç”Ÿæ´»': 'åˆ†äº«è¿åŠ¨ã€é¥®é£Ÿã€å¿ƒç†è°ƒé€‚ç­‰å¥åº·ç®¡ç†ç»éªŒã€‚',
            'åˆ›ä½œå¹³å°': 'å‰–æå†…å®¹åˆ›ä½œå›°å¢ƒä¸å¹³å°æœºåˆ¶ï¼Œé€‚åˆåˆ›ä½œè€…å‚è€ƒã€‚'
        }
        
        for theme, clip_ids in pre_clusters.items():
            # é™åˆ¶æ¯ä¸ªåˆé›†çš„ç‰‡æ®µæ•°é‡
            if len(clip_ids) > MAX_CLIPS_PER_COLLECTION:
                clip_ids = clip_ids[:MAX_CLIPS_PER_COLLECTION]
            
            collections.append({
                'id': str(collection_id),
                'collection_title': theme_titles.get(theme, theme),
                'collection_summary': theme_summaries.get(theme, f'{theme}ç›¸å…³ç²¾å½©ç‰‡æ®µåˆé›†'),
                'clip_ids': clip_ids
            })
            collection_id += 1
        
        return collections
    
    def _validate_collections(self, collections_data: List[Dict], clips_with_titles: List[Dict]) -> List[Dict]:
        """
        éªŒè¯å’Œæ¸…ç†åˆé›†æ•°æ®
        
        Args:
            collections_data: åŸå§‹åˆé›†æ•°æ®
            clips_with_titles: ç‰‡æ®µæ•°æ®
            
        Returns:
            éªŒè¯åçš„åˆé›†æ•°æ®
        """
        logger.info(f"ğŸ” [å¼€å§‹éªŒè¯] éªŒè¯åˆé›†æ•°æ®ï¼ŒåŸå§‹åˆé›†æ•°é‡: {len(collections_data) if collections_data else 0}")
        
        # è®°å½•æ‰€æœ‰å®é™…å¯ç”¨çš„ç‰‡æ®µæ ‡é¢˜ä¾›è°ƒè¯•
        all_actual_titles = []
        for clip in clips_with_titles:
            generated_title = clip.get('generated_title', clip['outline'])
            outline = clip['outline']
            all_actual_titles.append({
                'generated_title': generated_title,
                'outline': outline,
                'id': clip['id']
            })
        
        logger.debug(f"ğŸ“š [å®é™…ç‰‡æ®µæ ‡é¢˜] æ‰€æœ‰å¯ç”¨ç‰‡æ®µæ ‡é¢˜: {json.dumps(all_actual_titles, ensure_ascii=False, indent=2)}")
        
        # æ£€æŸ¥collections_dataæ˜¯å¦ä¸ºNoneæˆ–ä¸æ˜¯åˆ—è¡¨
        if collections_data is None:
            logger.warning("âš ï¸ [éªŒè¯æ•°æ®ä¸ºç©º] collections_dataä¸ºNone")
            return []
        
        if not isinstance(collections_data, list):
            logger.warning(f"âš ï¸ [éªŒè¯æ•°æ®ç±»å‹é”™è¯¯] collections_dataä¸æ˜¯åˆ—è¡¨ç±»å‹ï¼Œå®é™…ç±»å‹: {type(collections_data)}")
            logger.debug(f"ğŸ“„ [éªŒè¯æ•°æ®å†…å®¹]: {str(collections_data)[:1000]}")
            return []
        
        logger.debug(f"ğŸ“„ [åŸå§‹åˆé›†æ•°æ®]: {json.dumps(collections_data, ensure_ascii=False, indent=2)[:1000]}...")
        
        validated_collections = []
        
        for i, collection in enumerate(collections_data):
            try:
                logger.info(f"ğŸ” [éªŒè¯åˆé›† {i}] å¼€å§‹éªŒè¯ç¬¬ {i} ä¸ªåˆé›†")
                logger.debug(f"ğŸ“„ [åˆé›† {i} åŸå§‹æ•°æ®]: {json.dumps(collection, ensure_ascii=False, indent=2) if isinstance(collection, dict) else str(collection)}")
                
                # æ£€æŸ¥collectionæ˜¯å¦ä¸ºå­—å…¸ç±»å‹
                if not isinstance(collection, dict):
                    logger.warning(f"âš ï¸ [åˆé›† {i} ç±»å‹é”™è¯¯] åˆé›†æ•°æ®ä¸æ˜¯å­—å…¸ç±»å‹ï¼Œå®é™…ç±»å‹: {type(collection)}")
                    continue
                
                # éªŒè¯å¿…éœ€å­—æ®µ
                required_fields = ['collection_title', 'collection_summary', 'clips']
                missing_fields = [key for key in required_fields if key not in collection]
                
                if missing_fields:
                    logger.warning(f"âš ï¸ [åˆé›† {i} ç¼ºå°‘å­—æ®µ] ç¼ºå°‘å¿…éœ€å­—æ®µ: {missing_fields}")
                    logger.debug(f"ğŸ“„ [åˆé›† {i} å®é™…å­—æ®µ]: {list(collection.keys())}")
                    logger.debug(f"ğŸ“„ [åˆé›† {i} å®Œæ•´æ•°æ®]: {json.dumps(collection, ensure_ascii=False, indent=2)}")
                    continue
                
                logger.info(f"âœ… [åˆé›† {i} å­—æ®µéªŒè¯é€šè¿‡] åŒ…å«æ‰€æœ‰å¿…éœ€å­—æ®µ")
                
                # éªŒè¯ç‰‡æ®µåˆ—è¡¨
                clip_titles = collection['clips']
                logger.info(f"ğŸ” [åˆé›† {i} ç‰‡æ®µéªŒè¯] ç‰‡æ®µæ ‡é¢˜æ•°é‡: {len(clip_titles) if isinstance(clip_titles, list) else 'N/A'}")
                logger.debug(f"ğŸ“„ [åˆé›† {i} ç‰‡æ®µæ ‡é¢˜]: {clip_titles}")
                
                # æ£€æŸ¥clipsæ˜¯å¦ä¸ºåˆ—è¡¨ç±»å‹
                if not isinstance(clip_titles, list):
                    logger.warning(f"âš ï¸ [åˆé›† {i} ç‰‡æ®µç±»å‹é”™è¯¯] clipså­—æ®µä¸æ˜¯åˆ—è¡¨ç±»å‹ï¼Œå®é™…ç±»å‹: {type(clip_titles)}")
                    logger.debug(f"ğŸ“„ [åˆé›† {i} clipså­—æ®µå†…å®¹]: {str(clip_titles)}")
                    continue
                
                valid_clip_ids = []
                
                for j, clip_title in enumerate(clip_titles):
                    logger.debug(f"ğŸ” [åˆé›† {i} ç‰‡æ®µ {j}] æŸ¥æ‰¾ç‰‡æ®µæ ‡é¢˜: '{clip_title}'")
                    # æ ¹æ®æ ‡é¢˜æ‰¾åˆ°å¯¹åº”çš„ç‰‡æ®µID
                    found_clip = None
                    
                    # æ¸…ç†LLMè¿”å›çš„æ ‡é¢˜ï¼ˆå»é™¤å¤šä½™ç©ºæ ¼å’Œç‰¹æ®Šå­—ç¬¦ï¼‰
                    cleaned_clip_title = clip_title.strip()
                    # å»é™¤å¯èƒ½çš„å¼•å·
                    if cleaned_clip_title.startswith('"') and cleaned_clip_title.endswith('"'):
                        cleaned_clip_title = cleaned_clip_title[1:-1]
                    if cleaned_clip_title.startswith("'") and cleaned_clip_title.endswith("'"):
                        cleaned_clip_title = cleaned_clip_title[1:-1]
                    
                    # å¤„ç†å¯èƒ½çš„Unicodeå­—ç¬¦é—®é¢˜
                    import unicodedata
                    cleaned_clip_title = unicodedata.normalize('NFKC', cleaned_clip_title)
                    
                    logger.debug(f"   æ¸…ç†åæ ‡é¢˜: '{cleaned_clip_title}'")
                    logger.debug(f"   åŸå§‹æ ‡é¢˜ASCIIç : {[ord(c) for c in clip_title]}")
                    logger.debug(f"   æ¸…ç†åæ ‡é¢˜ASCIIç : {[ord(c) for c in cleaned_clip_title]}")
                    
                    # è®°å½•æ‰€æœ‰å®é™…ç‰‡æ®µæ ‡é¢˜çš„è¯¦ç»†ä¿¡æ¯
                    logger.debug(f"   å®é™…ç‰‡æ®µæ ‡é¢˜åˆ—è¡¨:")
                    for k, clip in enumerate(clips_with_titles):
                        generated_title = clip.get('generated_title', clip['outline'])
                        outline = clip['outline']
                        # æ¸…ç†å®é™…çš„æ ‡é¢˜
                        cleaned_generated_title = generated_title.strip()
                        cleaned_outline = outline.strip()
                        
                        # å¤„ç†Unicodeå­—ç¬¦
                        cleaned_generated_title = unicodedata.normalize('NFKC', cleaned_generated_title)
                        cleaned_outline = unicodedata.normalize('NFKC', cleaned_outline)
                        
                        logger.debug(f"     ç‰‡æ®µ{k} - Generated: '{generated_title}' (æ¸…ç†å: '{cleaned_generated_title}')")
                        logger.debug(f"     ç‰‡æ®µ{k} - Outline: '{outline}' (æ¸…ç†å: '{cleaned_outline}')")
                        logger.debug(f"     ç‰‡æ®µ{k} - Generated ASCII: {[ord(c) for c in generated_title]}")
                        logger.debug(f"     ç‰‡æ®µ{k} - Outline ASCII: {[ord(c) for c in outline]}")
                    
                    # å°è¯•å¤šç§åŒ¹é…ç­–ç•¥
                    for clip in clips_with_titles:
                        generated_title = clip.get('generated_title', clip['outline'])
                        outline = clip['outline']
                        
                        # æ¸…ç†å®é™…çš„æ ‡é¢˜
                        cleaned_generated_title = generated_title.strip()
                        cleaned_outline = outline.strip()
                        
                        # å¤„ç†Unicodeå­—ç¬¦
                        cleaned_generated_title = unicodedata.normalize('NFKC', cleaned_generated_title)
                        cleaned_outline = unicodedata.normalize('NFKC', cleaned_outline)
                        
                        logger.debug(f"   æ¯”è¾ƒ: '{cleaned_clip_title}' vs '{cleaned_generated_title}' | '{cleaned_outline}'")
                        
                        # ç­–ç•¥1: ç²¾ç¡®åŒ¹é…ï¼ˆå¿½ç•¥é¦–å°¾ç©ºæ ¼ï¼‰
                        if (cleaned_clip_title.strip() == cleaned_generated_title.strip() or 
                            cleaned_clip_title.strip() == cleaned_outline.strip()):
                            found_clip = clip
                            logger.info(f"âœ… [åˆé›† {i} ç‰‡æ®µ {j} ç²¾ç¡®åŒ¹é…æˆåŠŸ] ä½¿ç”¨ç²¾ç¡®åŒ¹é…æ‰¾åˆ°ç‰‡æ®µ")
                            break
                        
                        # ç­–ç•¥2: å»é™¤æ ‡ç‚¹ç¬¦å·ååŒ¹é…
                        import re
                        def remove_punctuation(text):
                            # ç§»é™¤å¸¸è§çš„ä¸­æ–‡å’Œè‹±æ–‡æ ‡ç‚¹ç¬¦å·
                            punctuation_pattern = r"""[ï¼Œã€‚ï¼ï¼Ÿï¼›ï¼š""''ï¼ˆï¼‰ã€ã€‘ã€Šã€‹ã€\s\.\,\!\?\;\:\"\''\(\)\[\]\<\>]*"""
                            return re.sub(punctuation_pattern, '', text).strip()
                        
                        no_punct_clip_title = remove_punctuation(cleaned_clip_title)
                        no_punct_generated_title = remove_punctuation(cleaned_generated_title)
                        no_punct_outline = remove_punctuation(cleaned_outline)
                        
                        logger.debug(f"   å»æ ‡ç‚¹æ¯”è¾ƒ: '{no_punct_clip_title}' vs '{no_punct_generated_title}' | '{no_punct_outline}'")
                        
                        if (no_punct_clip_title == no_punct_generated_title or 
                            no_punct_clip_title == no_punct_outline):
                            found_clip = clip
                            logger.info(f"âœ… [åˆé›† {i} ç‰‡æ®µ {j} å»æ ‡ç‚¹åŒ¹é…æˆåŠŸ] ä½¿ç”¨å»æ ‡ç‚¹åŒ¹é…æ‰¾åˆ°ç‰‡æ®µ")
                            break
                        
                        # ç­–ç•¥3: åŒ…å«åŒ¹é…ï¼ˆLLMæ ‡é¢˜åŒ…å«åœ¨å®é™…æ ‡é¢˜ä¸­ï¼Œæˆ–å®é™…æ ‡é¢˜åŒ…å«åœ¨LLMæ ‡é¢˜ä¸­ï¼‰
                        if (no_punct_clip_title in no_punct_generated_title or 
                            no_punct_generated_title in no_punct_clip_title or
                            no_punct_clip_title in no_punct_outline or 
                            no_punct_outline in no_punct_clip_title):
                            found_clip = clip
                            logger.info(f"âœ… [åˆé›† {i} ç‰‡æ®µ {j} åŒ…å«åŒ¹é…æˆåŠŸ] ä½¿ç”¨åŒ…å«åŒ¹é…æ‰¾åˆ°ç‰‡æ®µ")
                            logger.debug(f"     åŒ…å«å…³ç³»: '{no_punct_clip_title}' in '{no_punct_generated_title}' or '{no_punct_generated_title}' in '{no_punct_clip_title}' or '{no_punct_clip_title}' in '{no_punct_outline}' or '{no_punct_outline}' in '{no_punct_clip_title}'")
                            break
                        
                        # ç­–ç•¥4: æ¨¡ç³ŠåŒ¹é…ï¼ˆå¿½ç•¥æ ‡ç‚¹å’Œç©ºæ ¼ï¼‰
                        def normalize_text(text):
                            # ç§»é™¤æ ‡ç‚¹ç¬¦å·å’Œå¤šä½™ç©ºæ ¼ï¼Œè½¬æ¢ä¸ºå°å†™
                            return re.sub(r'[^\w\s]', '', text).strip().lower()
                        
                        normalized_clip_title = normalize_text(cleaned_clip_title)
                        normalized_generated_title = normalize_text(cleaned_generated_title)
                        normalized_outline = normalize_text(cleaned_outline)
                        
                        logger.debug(f"   æ¨¡ç³Šæ¯”è¾ƒ: '{normalized_clip_title}' vs '{normalized_generated_title}' | '{normalized_outline}'")
                        
                        if (normalized_clip_title == normalized_generated_title or 
                            normalized_clip_title == normalized_outline):
                            found_clip = clip
                            logger.info(f"âœ… [åˆé›† {i} ç‰‡æ®µ {j} æ¨¡ç³ŠåŒ¹é…æˆåŠŸ] ä½¿ç”¨æ¨¡ç³ŠåŒ¹é…æ‰¾åˆ°ç‰‡æ®µ")
                            break
                    
                    if found_clip:
                        valid_clip_ids.append(found_clip['id'])
                        logger.debug(f"âœ… [åˆé›† {i} ç‰‡æ®µ {j} åŒ¹é…æˆåŠŸ] æ‰¾åˆ°åŒ¹é…ç‰‡æ®µ ID: {found_clip['id']}")
                        logger.debug(f"   åŒ¹é…è¯¦æƒ… - LLMæ ‡é¢˜: '{clip_title}', å®é™…æ ‡é¢˜: '{found_clip.get('generated_title', found_clip['outline'])}'")
                    else:
                        logger.warning(f"âš ï¸ [åˆé›† {i} ç‰‡æ®µ {j} åŒ¹é…å¤±è´¥] æœªæ‰¾åˆ°åŒ¹é…çš„ç‰‡æ®µ: '{clip_title}'")
                        # è®°å½•æ‰€æœ‰å®é™…æ ‡é¢˜ä¾›è°ƒè¯•
                        all_titles = [clip.get('generated_title', clip['outline']) for clip in clips_with_titles]
                        logger.debug(f"   å¯ç”¨æ ‡é¢˜åˆ—è¡¨: {all_titles}")
                        # ä¹Ÿè®°å½•æ¸…ç†åçš„æ ‡é¢˜
                        cleaned_titles = [title.strip() for title in all_titles]
                        logger.debug(f"   æ¸…ç†åæ ‡é¢˜åˆ—è¡¨: {cleaned_titles}")
                
                if len(valid_clip_ids) < 2:
                    logger.warning(f"âš ï¸ [åˆé›† {i} ç‰‡æ®µä¸è¶³] æœ‰æ•ˆç‰‡æ®µå°‘äº2ä¸ª ({len(valid_clip_ids)}ä¸ª)ï¼Œè·³è¿‡")
                    continue
                
                # é™åˆ¶æ¯ä¸ªåˆé›†çš„ç‰‡æ®µæ•°é‡
                if len(valid_clip_ids) > MAX_CLIPS_PER_COLLECTION:
                    logger.info(f"âœ‚ï¸ [åˆé›† {i} ç‰‡æ®µè¶…é™] ç‰‡æ®µæ•°é‡ {len(valid_clip_ids)} è¶…è¿‡é™åˆ¶ {MAX_CLIPS_PER_COLLECTION}ï¼Œæˆªå–å‰{MAX_CLIPS_PER_COLLECTION}ä¸ª")
                    valid_clip_ids = valid_clip_ids[:MAX_CLIPS_PER_COLLECTION]
                
                validated_collection = {
                    'id': str(i + 1),
                    'collection_title': collection['collection_title'],
                    'collection_summary': collection['collection_summary'],
                    'clip_ids': valid_clip_ids
                }
                
                logger.info(f"âœ… [åˆé›† {i} éªŒè¯é€šè¿‡] æ ‡é¢˜: '{validated_collection['collection_title']}', ç‰‡æ®µæ•°: {len(valid_clip_ids)}")
                validated_collections.append(validated_collection)
                
            except Exception as e:
                logger.error(f"âŒ [éªŒè¯åˆé›† {i} å¤±è´¥] é”™è¯¯: {str(e)}")
                logger.exception(e)
                continue
        
        logger.info(f"âœ… [éªŒè¯å®Œæˆ] æœ€ç»ˆæœ‰æ•ˆåˆé›†æ•°é‡: {len(validated_collections)}")
        return validated_collections
    
    def _create_default_collections(self, clips_with_titles: List[Dict]) -> List[Dict]:
        """
        åˆ›å»ºé»˜è®¤åˆé›†ï¼ˆå½“èšç±»å¤±è´¥æ—¶ï¼‰
        
        Args:
            clips_with_titles: ç‰‡æ®µæ•°æ®
            
        Returns:
            é»˜è®¤åˆé›†æ•°æ®
        """
        logger.info("åˆ›å»ºé»˜è®¤åˆé›†...")
        
        # æŒ‰è¯„åˆ†åˆ†ç»„
        high_score = []
        medium_score = []
        
        for clip in clips_with_titles:
            score = clip.get('final_score', 0)
            if score >= 0.8:
                high_score.append(clip)
            elif score >= 0.6:
                medium_score.append(clip)
        
        collections = []
        
        # åˆ›å»ºé«˜åˆ†åˆé›†
        if len(high_score) >= 2:
            collections.append({
                'id': '1',
                'collection_title': 'ç²¾é€‰é«˜åˆ†ç‰‡æ®µ',
                'collection_summary': 'è¯„åˆ†æœ€é«˜çš„ç²¾å½©ç‰‡æ®µåˆé›†',
                'clip_ids': [clip['id'] for clip in high_score[:MAX_CLIPS_PER_COLLECTION]]
            })
        
        # åˆ›å»ºä¸­ç­‰åˆ†åˆé›†
        if len(medium_score) >= 2:
            collections.append({
                'id': '2',
                'collection_title': 'ä¼˜è´¨å†…å®¹æ¨è',
                'collection_summary': 'ç²¾é€‰ä¼˜è´¨å†…å®¹ç‰‡æ®µ',
                'clip_ids': [clip['id'] for clip in medium_score[:MAX_CLIPS_PER_COLLECTION]]
            })
        
        return collections
    
    def save_collections(self, collections_data: List[Dict], output_path: Optional[Path] = None) -> Path:
        """
        ä¿å­˜åˆé›†æ•°æ®
        
        Args:
            collections_data: åˆé›†æ•°æ®
            output_path: è¾“å‡ºè·¯å¾„
            
        Returns:
            ä¿å­˜çš„æ–‡ä»¶è·¯å¾„
        """
        if output_path is None:
            output_path = self.metadata_dir / "collections.json"
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        # ä¿å­˜æ•°æ®
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(collections_data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"åˆé›†æ•°æ®å·²ä¿å­˜åˆ°: {output_path}")
        return output_path
    
    def load_collections(self, input_path: Path) -> List[Dict]:
        """
        ä»æ–‡ä»¶åŠ è½½åˆé›†æ•°æ®
        
        Args:
            input_path: è¾“å…¥æ–‡ä»¶è·¯å¾„
            
        Returns:
            åˆé›†æ•°æ®
        """
        with open(input_path, 'r', encoding='utf-8') as f:
            return json.load(f)

def run_step5_clustering(clips_with_titles_path: Path, output_path: Optional[Path] = None, metadata_dir: Optional[str] = None, prompt_files: Dict = None) -> List[Dict]:
    """
    è¿è¡ŒStep 5: ä¸»é¢˜èšç±»
    
    Args:
        clips_with_titles_path: å¸¦æ ‡é¢˜çš„ç‰‡æ®µæ–‡ä»¶è·¯å¾„
        output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        prompt_files: è‡ªå®šä¹‰æç¤ºè¯æ–‡ä»¶
        
    Returns:
        åˆé›†æ•°æ®
    """
    # åŠ è½½æ•°æ®
    with open(clips_with_titles_path, 'r', encoding='utf-8') as f:
        clips_with_titles = json.load(f)
    
    # åˆ›å»ºèšç±»å™¨
    if metadata_dir is None:
        metadata_dir = METADATA_DIR
    clusterer = ClusteringEngine(metadata_dir=Path(metadata_dir), prompt_files=prompt_files)
    
    # è¿›è¡Œèšç±»
    collections_data = clusterer.cluster_clips(clips_with_titles)
    
    # ä¿å­˜ç»“æœ
    if output_path is None:
        if metadata_dir is None:
            metadata_dir = METADATA_DIR
        output_path = Path(metadata_dir) / "step5_collections.json"
    
    clusterer.save_collections(collections_data, output_path)
    
    return collections_data
